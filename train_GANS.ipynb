{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c83bb",
   "metadata": {
    "id": "bf8c83bb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd.variable import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch import from_numpy\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ab571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Vars\n",
    "model_path = 'Simpsons_train3/'\n",
    "dataset_path = 'image_dataset/Simpsons/perfect-noback/'\n",
    "\n",
    "dim = 64\n",
    "dim_noise = 120\n",
    "\n",
    "batchsize = 32\n",
    "num_epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a10004",
   "metadata": {
    "id": "f0a10004",
    "outputId": "0fb205ed-a025-4a3d-c6a6-a01752b26904",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load and Resize Images\n",
    "\n",
    "resize = (dim, dim)\n",
    "data_img = []\n",
    "for f in os.listdir(dataset_path):\n",
    "    img = Image.open(dataset_path+f) # image extension *.png,*.jpg\n",
    "    new_width  = resize[0]\n",
    "    new_height = resize[1]\n",
    "    img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "    array_image = np.array(img)[:, :, :3]\n",
    "    data_img.append(np.array(array_image).transpose(2,0,1))\n",
    "    \n",
    "# Normalize data [-1, 1]\n",
    "data_x = (np.array(data_img)-127.5)/127.5\n",
    "\n",
    "print('ALL DATA SHAPE:')\n",
    "print(data_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e150dc04",
   "metadata": {
    "id": "e150dc04",
    "outputId": "7dec0645-00c9-4d37-f2cf-0ff757c308ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Init Dataset and DataLoader\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = data_x.shape[0]\n",
    "        self.x_data = np.float32(data_x)\n",
    "        self.y_data = from_numpy(data_x[:, [0], [0]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "dataset = MyDataset()\n",
    "my_data_loader = DataLoader(dataset=dataset, batch_size=batchsize, shuffle=True)\n",
    "print('NUMBER OF BATCHES')\n",
    "print(len(my_data_loader))\n",
    "\n",
    "# ONE BATCH\n",
    "images, labels = next(iter(my_data_loader))\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e5639",
   "metadata": {
    "id": "ce2e5639",
    "outputId": "7087da1b-ba09-4b14-b1d8-5bef59b70bf7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Functions\n",
    "\n",
    "def plot_images(images):\n",
    "  figure = plt.figure(figsize=(3, 3))\n",
    "  cols, rows = 3, 3\n",
    "  \n",
    "  for i in range(1, cols * rows + 1):\n",
    "    if len(images) > cols*rows:\n",
    "      rand_i = i - 1\n",
    "    else:\n",
    "      rand_i = random.randint(0, len(images)-1)\n",
    "    img = images[rand_i]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\")\n",
    "    img = img * 127.5 + 127.5\n",
    "    plt.imshow(np.rot90(img.T, 3).astype(np.uint8), cmap=\"PRGn\")\n",
    "\n",
    "def plot_image(img):\n",
    "    print(img.shape)\n",
    "    figure = plt.figure(figsize=(3, 3))\n",
    "    img = img * 127.5 + 127.5\n",
    "    plt.imshow(np.rot90(img.T, 3).astype(np.uint8), cmap=\"PRGn\")\n",
    "\n",
    "# TEST IMAGES\n",
    "for i in range(2):\n",
    "  print(i)\n",
    "  batchh = next(iter(my_data_loader))\n",
    "  plot_images(batchh[0])\n",
    "  plt.show()\n",
    "batchh = next(iter(my_data_loader))\n",
    "plot_image(batchh[0][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3964e2",
   "metadata": {
    "id": "5c3964e2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Init Generator and Discriminator Nets\n",
    "\n",
    "class DiscriminatorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer discriminative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "        n_features = dim*dim*3\n",
    "        n_out = 1\n",
    "        factor_n = 0.5\n",
    "        \n",
    "        self.hidden0 = nn.Sequential( \n",
    "            nn.Linear(n_features, int(factor_n*1024)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(int(factor_n*1024), int(factor_n*512)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(int(factor_n*512), int(factor_n*256)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            torch.nn.Linear(int(factor_n*256), n_out),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GeneratorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer generative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        n_features = dim_noise\n",
    "        n_out = dim*dim*3\n",
    "        factor_n = 1\n",
    "        \n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_features, int(256*factor_n)),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(            \n",
    "            nn.Linear(int(256*factor_n), int(512*factor_n)),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(int(512*factor_n), int(1024*factor_n)),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(int(1024*factor_n), n_out),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate Generator and Discriminator\n",
    "generator = GeneratorNet()\n",
    "discriminator = DiscriminatorNet()\n",
    "\n",
    "# Optimizers\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.00015)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.00015)\n",
    "\n",
    "# Loss fucntion\n",
    "loss = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c2a6f",
   "metadata": {
    "id": "e48c2a6f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random noise sampler\n",
    "def noise(size):\n",
    "    n = Variable(torch.randn(size, dim_noise))\n",
    "    return n\n",
    "\n",
    "# Testing noise sampler\n",
    "test_noise = noise(9)\n",
    "test_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd4d7c",
   "metadata": {
    "id": "7dbd4d7c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert images to vectors and vice versa\n",
    "def images_to_vectors(images):\n",
    "    return images.view(images.size(0), dim*dim*3)\n",
    "def vectors_to_images(vectors):\n",
    "    return vectors.view(vectors.size(0), 3, dim, dim)\n",
    "\n",
    "# Target vectors\n",
    "def ones_target(size):\n",
    "    data = Variable(torch.ones(size, 1))\n",
    "    return data\n",
    "def zeros_target(size):\n",
    "    data = Variable(torch.zeros(size, 1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5586d",
   "metadata": {
    "id": "1bf5586d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN FUNCTIONS\n",
    "\n",
    "# Discriminator Trainer\n",
    "def train_discriminator(optimizer, real_data, fake_data):\n",
    "    N = real_data.size(0)\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1.1 Train on Real Data\n",
    "    prediction_real = discriminator(real_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_real = loss(prediction_real, ones_target(N))\n",
    "    error_real.backward()\n",
    "\n",
    "    # 1.2 Train on Fake Data\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_fake = loss(prediction_fake, zeros_target(N))\n",
    "    error_fake.backward()\n",
    "    \n",
    "    # 1.3 Update weights with gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error and predictions for real and fake inputs\n",
    "    return error_real + error_fake, prediction_real, prediction_fake\n",
    "\n",
    "# Generator Trainer\n",
    "def train_generator(optimizer, fake_data):\n",
    "    N = fake_data.size(0)\n",
    "\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Sample noise and generate fake data\n",
    "    prediction = discriminator(fake_data)\n",
    "\n",
    "    # Calculate error and backpropagate\n",
    "    error = loss(prediction, ones_target(N))\n",
    "    error.backward()\n",
    "\n",
    "    # Update weights with gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TRAIN Directories\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "result_img_path = model_path+'train_images/'\n",
    "if not os.path.exists(model_path):\n",
    "  os.makedirs(model_path)\n",
    "if not os.path.exists(result_img_path):\n",
    "  os.makedirs(result_img_path)\n",
    "print(f'TRAIN DIRECTORY: {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67b30d",
   "metadata": {
    "id": "3e67b30d",
    "outputId": "999b84f1-8c40-4c71-9286-818760837f2b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    print(f' ************** EPOCH {epoch} ************** ')\n",
    "    for n_batch, (real_batch,_) in enumerate(my_data_loader):\n",
    "        N = real_batch.size(0)\n",
    "\n",
    "        # 1. Train Discriminator\n",
    "        real_data = Variable(images_to_vectors(real_batch))\n",
    "\n",
    "        # Generate fake data and detach \n",
    "        # (so gradients are not calculated for generator)\n",
    "        fake_data = generator((noise(N).detach()))\n",
    "\n",
    "        # Train D\n",
    "        d_error, d_pred_real, d_pred_fake = \\\n",
    "              train_discriminator(d_optimizer, real_data, fake_data)\n",
    "\n",
    "        # 2. Train Generator\n",
    "        # Generate fake data\n",
    "        fake_data = generator(noise(N))\n",
    "\n",
    "        # Train G\n",
    "        g_error = train_generator(g_optimizer, fake_data)\n",
    "\n",
    "        \n",
    "    # SAVE AND SHOW DATA\n",
    "    if epoch % 1 == 0:\n",
    "        test_images = vectors_to_images(generator(test_noise))\n",
    "        test_images = test_images.data\n",
    "        plot_images(test_images)\n",
    "        plt.savefig(result_img_path+str(epoch)+'_epoch.png')\n",
    "        plt.show()\n",
    "        pickle.dump(generator, open(model_path+'generator.model', 'wb'))\n",
    "        print('DISCRIMINATOR ERROR')\n",
    "        print(d_error)\n",
    "        print('GENERATOR ERROR')\n",
    "        print(g_error)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
